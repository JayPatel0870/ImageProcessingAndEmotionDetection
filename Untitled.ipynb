{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d83d90-b26f-416d-96c4-36b415a52a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.16.1-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.1->torchvision) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.1->torchvision) (2023.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.1.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch==2.1.1->torchvision) (1.3.0)\n",
      "Using cached torchvision-0.16.1-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f06503-958f-408b-bf66-49b427900095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: skorch in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (1.11.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.14.0 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from skorch) (4.66.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.22.0->skorch) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.22.0->skorch) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jaybp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.14.0->skorch) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install skorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset\n",
    "from skorch import NeuralNetClassifier\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "705c983d-779c-42a5-9b35-700ded9cbfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1509\n",
      "323\n",
      "324\n",
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 138\u001b[0m\n\u001b[0;32m    126\u001b[0m net \u001b[38;5;241m=\u001b[39m NeuralNetClassifier(\n\u001b[0;32m    127\u001b[0m     model,\n\u001b[0;32m    128\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Training for the current fold\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset_fold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Testing for the current fold\u001b[39;00m\n\u001b[0;32m    141\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mscore(test_dataset_fold, y\u001b[38;5;241m=\u001b[39my_test_fold)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skorch\\classifier.py:165\u001b[0m, in \u001b[0;36mNeuralNetClassifier.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# this is actually a pylint bug:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNeuralNetClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skorch\\net.py:1317\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize and fit the module.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \n\u001b[0;32m   1287\u001b[0m \u001b[38;5;124;03mIf the module was already initialized, by calling fit, the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \n\u001b[0;32m   1315\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_:\n\u001b[1;32m-> 1317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skorch\\net.py:905\u001b[0m, in \u001b[0;36mNeuralNet.initialize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_module()\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_criterion()\n\u001b[1;32m--> 905\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_history()\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skorch\\net.py:876\u001b[0m, in \u001b[0;36mNeuralNet._initialize_optimizer\u001b[1;34m(self, reason)\u001b[0m\n\u001b[0;32m    873\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_reinit_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, triggered_directly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28mprint\u001b[39m(msg)\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;66;03m# register the virtual params for all optimizers\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skorch\\net.py:642\u001b[0m, in \u001b[0;36mNeuralNet.initialize_optimizer\u001b[1;34m(self, triggered_directly)\u001b[0m\n\u001b[0;32m    638\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params_for_optimizer(\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m, named_parameters)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;66;03m# pylint: disable=attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:261\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    259\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    263\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_classes = 4\n",
    "learning_rate = 0.001\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "local_dataset_path = \"dataset\"\n",
    "dataset = ImageFolder(root=local_dataset_path, transform=transform)\n",
    "# Define the sizes of training and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size,val_size])\n",
    "classes = ('angry','focused','neutral','tired')\n",
    "y_train = np.array([y for x, y in iter(train_dataset)])\n",
    "print(len(y_train))\n",
    "y_val = np.array([y for x, y in iter(val_dataset)])\n",
    "print(len(y_val))\n",
    "\n",
    "y_test = np.array([y for x, y in iter(test_dataset)])\n",
    "print(len(y_test))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(CNN, self)._init_()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # Calculate output size after each layer\n",
    "        out_size = lambda i, k, p, s: (i - k + 2 * p) // s + 1\n",
    "\n",
    "        out1 = out_size(96, 1, 1, 1)  # Output size after conv1\n",
    "        out2 = out_size(out1, 2, 0, 2)  # Output size after pool1\n",
    "        out3 = out_size(out2, 3, 1, 1)  # Output size after conv2\n",
    "        out4 = out_size(out3, 2, 0, 2)  # Output size after pool2\n",
    "\n",
    "        expected_input_size = out4 * out4 * 64  # Assuming 64 channels in the last conv layer\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(expected_input_size, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "net = NeuralNetClassifier(\n",
    "    model,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    optimizer = torch.optim.Adam,\n",
    "    max_epochs=num_epochs,\n",
    "    lr=learning_rate,\n",
    "    batch_size=32,\n",
    "    iterator_train__shuffle=True,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "# Initialize k-fold cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming the dataset and other configurations are already defined\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics across folds\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "conf_matrices = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n",
    "    print(f'\\nFold {fold + 1}/{num_folds}')\n",
    "\n",
    "    # Extract train and test datasets for the current fold\n",
    "    train_dataset_fold = Subset(dataset, train_index)\n",
    "    test_dataset_fold = Subset(dataset, test_index)\n",
    "\n",
    "    # Extract labels for the current fold\n",
    "    y_train_fold = np.array([y for _, y in iter(train_dataset_fold)])\n",
    "    y_test_fold = np.array([y for _, y in iter(test_dataset_fold)])\n",
    "\n",
    "    # Define the model for each fold\n",
    "    model = CNN()\n",
    "    net = NeuralNetClassifier(\n",
    "        model,\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.Adam,\n",
    "        max_epochs=num_epochs,\n",
    "        lr=learning_rate,\n",
    "        batch_size=32,\n",
    "        iterator_train__shuffle=True,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    # Training for the current fold\n",
    "    net.fit(train_dataset_fold, y=y_train_fold)\n",
    "\n",
    "    # Testing for the current fold\n",
    "    test_acc = net.score(test_dataset_fold, y=y_test_fold)\n",
    "    print(f'Test Accuracy: {test_acc * 100:.2f}%')\n",
    "\n",
    "    y_pred = net.predict(test_dataset_fold)\n",
    "    accuracy_list.append(accuracy_score(y_test_fold, y_pred))\n",
    "\n",
    "    # Store confusion matrix for each fold\n",
    "    conf_matrices.append(confusion_matrix(y_test_fold, y_pred))\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for each fold\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test_fold, y_pred, average='weighted', zero_division=1)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = np.mean(accuracy_list)\n",
    "avg_precision = np.mean(precision_list)\n",
    "avg_recall = np.mean(recall_list)\n",
    "avg_f1 = np.mean(f1_list)\n",
    "\n",
    "# Print average metrics\n",
    "print(f'Average Accuracy: {avg_accuracy * 100:.2f}%')\n",
    "print(f'Average Precision: {avg_precision:.2f}')\n",
    "print(f'Average Recall: {avg_recall:.2f}')\n",
    "print(f'Average F1-score: {avg_f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63c2339a-a986-4b17-86b8-5158548c257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (61,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 110\u001b[0m\n\u001b[0;32m    108\u001b[0m y_train_fold \u001b[38;5;241m=\u001b[39m [y\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m _, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(train_loader)]\n\u001b[0;32m    109\u001b[0m y_test_fold \u001b[38;5;241m=\u001b[39m [y\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m _, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(test_loader)]\n\u001b[1;32m--> 110\u001b[0m y_train_fold \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m y_test_fold \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_test_fold)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Define the model for each fold\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (61,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from skorch import NeuralNetClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Constants\n",
    "num_epochs = 10\n",
    "num_classes = 4\n",
    "learning_rate = 0.001\n",
    "local_dataset_path = \"dataset\"\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = ImageFolder(root=local_dataset_path, transform=transform)\n",
    "\n",
    "# Define the sizes of training and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n",
    "\n",
    "# Model definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # Calculate output size after each layer\n",
    "        out_size = lambda i, k, p, s: (i - k + 2 * p) // s + 1\n",
    "\n",
    "        out1 = out_size(96, 1, 1, 1)  # Output size after conv1\n",
    "        out2 = out_size(out1, 2, 0, 2)  # Output size after pool1\n",
    "        out3 = out_size(out2, 3, 1, 1)  # Output size after conv2\n",
    "        out4 = out_size(out3, 2, 0, 2)  # Output size after pool2\n",
    "\n",
    "        expected_input_size = out4 * out4 * 64  # Assuming 64 channels in the last conv layer\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(expected_input_size, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "# Initialize k-fold cross-validation\n",
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics across folds\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "conf_matrices = []\n",
    "\n",
    "# Inside the training loop\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(dataset)):\n",
    "    print(f'\\nFold {fold + 1}/{num_folds}')\n",
    "\n",
    "    # Extract train and test datasets for the current fold\n",
    "    train_dataset_fold = Subset(dataset, train_index)\n",
    "    test_dataset_fold = Subset(dataset, test_index)\n",
    "\n",
    "    # Create DataLoader for training and testing\n",
    "    train_loader = DataLoader(train_dataset_fold, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset_fold, batch_size=32, shuffle=False)\n",
    "\n",
    "    y_train_fold = [y.numpy() for _, y in iter(train_loader)]\n",
    "    y_test_fold = [y.numpy() for _, y in iter(test_loader)]\n",
    "    y_train_fold = np.array(y_train_fold)\n",
    "    y_test_fold = np.array(y_test_fold)\n",
    "\n",
    "\n",
    "    # Define the model for each fold\n",
    "    model = CNN()\n",
    "    net = NeuralNetClassifier(\n",
    "        model,\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.Adam,\n",
    "        max_epochs=num_epochs,\n",
    "        lr=learning_rate,\n",
    "        batch_size=32,\n",
    "        iterator_train__shuffle=True,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    # Training for the current fold\n",
    "    net.fit(train_loader, y=y_train_fold)\n",
    "\n",
    "    # Testing for the current fold\n",
    "    test_acc = net.score(test_loader, y=y_test_fold)\n",
    "    print(f'Test Accuracy: {test_acc * 100:.2f}%')\n",
    "\n",
    "    y_pred = net.predict(test_loader)\n",
    "    accuracy_list.append(accuracy_score(y_test_fold, y_pred))\n",
    "\n",
    "    # Store confusion matrix for each fold\n",
    "    conf_matrices.append(confusion_matrix(y_test_fold, y_pred))\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for each fold\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test_fold, y_pred, average='weighted', zero_division=1)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = np.mean(accuracy_list)\n",
    "avg_precision = np.mean(precision_list)\n",
    "avg_recall = np.mean(recall_list)\n",
    "avg_f1 = np.mean(f1_list)\n",
    "\n",
    "# Print average metrics\n",
    "print(f'Average Accuracy: {avg_accuracy * 100:.2f}%')\n",
    "print(f'Average Precision: {avg_precision:.2f}')\n",
    "print(f'Average Recall: {avg_recall:.2f}')\n",
    "print(f'Average F1-score: {avg_f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942bea46-1573-4fb3-9e11-68d127f9d2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
